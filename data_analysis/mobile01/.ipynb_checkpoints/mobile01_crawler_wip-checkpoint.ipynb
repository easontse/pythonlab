{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import requests, time, csv, os, sys, logging\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def httpGet(url):\n",
    "    time.sleep(2.2)\n",
    "    logger.info('HTTP GET: ' + url)\n",
    "    return requests.get(url)\n",
    "\n",
    "def get_linklist(url):\n",
    "    logger.info('Generating Linklist on: ' + url)\n",
    "    \n",
    "    soup = BeautifulSoup(httpGet(url).text,'lxml')    \n",
    "    \n",
    "    linklist = []\n",
    "    \n",
    "    for i in soup.findAll('span', {\"class\":\"otherpages\"}):\n",
    "        raw_url = i.contents[-1]['href']\n",
    "        len_without_pageNum = int(27 + len(raw_url[raw_url.find('&t=')+3:raw_url.find('&p=')]))\n",
    "        tgt_url = raw_url[:len_without_pageNum-3]\n",
    "        if tgt_url not in linklist:\n",
    "            linklist.append(tgt_url)\n",
    "\n",
    "        tgt_url = raw_url[:len_without_pageNum]\n",
    "        pageNum = int(raw_url[-(len(raw_url)-len_without_pageNum):])\n",
    "        while pageNum > 1:\n",
    "            tgt_url_with_pgNum = tgt_url + str(pageNum)\n",
    "            if tgt_url_with_pgNum not in linklist:\n",
    "                linklist.append(tgt_url_with_pgNum)\n",
    "            pageNum -= 1\n",
    "\n",
    "    for i in soup.select('.subject'):\n",
    "        if '主題' not in i and i not in linklist:\n",
    "            linklist.append(i.a['href'])\n",
    "    \n",
    "    logger.info('Links in the list: ' + str(len(linklist)))\n",
    "    return linklist\n",
    "\n",
    "def crawl_content(url):\n",
    "    logger.info('Crawling Content on: ' + url)\n",
    "    res = httpGet(url)\n",
    "    \n",
    "    if url != res.url:\n",
    "        logger.warning('Requested URL does not match Response URL, return None!')\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    \n",
    "    date = []\n",
    "    for i in soup.find_all('div', {'class':'date'}):\n",
    "        date.append(i.text[:-4].strip())\n",
    "\n",
    "    author = []\n",
    "    for i in soup.find_all('div', {'class':'fn'}):\n",
    "        author.append(i.text.strip())\n",
    "\n",
    "    p_content = []\n",
    "    inchar = '\\n\\r'\n",
    "    ourchar = '  '\n",
    "    transchar = str.maketrans(inchar, ourchar)\n",
    "    for i in soup.find_all('div', {'class':'single-post-content'}):\n",
    "        p_content.append(i.text.translate(transchar).strip())\n",
    "    \n",
    "    dataset = []\n",
    "    for a,b,c in zip(date, author, p_content):\n",
    "        dataset.append([a, b, soup.select('.topic')[0].text, c, url])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "\n",
    "logging.basicConfig(filename='mobile01-luxgen.log',level=logging.INFO,format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    " \n",
    "pageNum = 1\n",
    "while True:\n",
    "\n",
    "    url = 'http://www.mobile01.com/topiclist.php?f=444&p=' + str(pageNum)\n",
    "    logger.info('Start Crawling on: ' + url + '\\n')\n",
    "    \n",
    "    if url != httpGet(url).url:\n",
    "        logger.error('Requested URL does not match Response URL, QUIT!')\n",
    "        break\n",
    "    \n",
    "    for i in get_linklist(url):\n",
    "        with open('mobile01-luxgen.csv', 'a') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            for x in crawl_content('http://www.mobile01.com/'+i):\n",
    "                if x != None:\n",
    "                    writer.writerow([x[0],x[1],x[2],x[3],x[4]])\n",
    "                else:\n",
    "                    logger.warning('None received, skipped!')\n",
    "                    continue\n",
    "    logger.info('Finished: ' + url + '\\n')\n",
    "    pageNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_soup = BeautifulSoup(httpGet('http://www.mobile01.com/topicdetail.php?f=444&t=4214870&p=2').text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(index_soup.select('.topic')[0].text)\n",
    "\n",
    "#print(index_soup.select('main'))\n",
    "\n",
    "date = []\n",
    "for i in index_soup.find_all('div', {'class':'date'}):\n",
    "    date.append(i.text[:-4].strip())\n",
    "\n",
    "author = []\n",
    "for i in index_soup.find_all('div', {'class':'fn'}):\n",
    "    author.append(i.text.strip())\n",
    "    \n",
    "post_content = []\n",
    "inchar = '\\n\\r'\n",
    "ourchar = '  '\n",
    "transchar = str.maketrans(inchar, ourchar)\n",
    "for i in index_soup.find_all('div', {'class':'single-post-content'}):\n",
    "    post_content.append(i.text.translate(transchar).strip())\n",
    "\n",
    "for i in zip(date, author, post_content):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
